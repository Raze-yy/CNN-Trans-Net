# ðŸŒˆ Donaldson Matrix Prediction using CNN - Transformer Model

This repository provides PyTorch implementations for predicting excitation-emission matrices (EEMs) of fluorescent materials using hybrid CNN and Transformer models. The model takes incident and emission spectra as input and predicts a full 2D EEM.



---

## ðŸ“ Project Structure

.
â”œâ”€â”€ models/ # All model definitions (CNN, Transformer variants)
â”œâ”€â”€ utils/ # Data loading and preprocessing utilities
â”œâ”€â”€ data/ # Sample input and groundtruth data
â”œâ”€â”€ train.py # Main training script
â”œâ”€â”€ plot_loss.py # Script to visualize training/validation loss
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # This file


---

## ðŸš€ Getting Started

### 1. Install dependencies

```

pip install -r requirements.txt
ðŸ“ Data Description
This repository includes only a sample dataset for Sample 1 to demonstrate input/output data structure.

Folder structure:

data/
â”œâ”€â”€ input/
â”‚   â””â”€â”€ 1.xlsx                  # Input excitation spectrum (Sample 1)
â”œâ”€â”€ groundtruth_d65/
â”‚   â””â”€â”€ Sample1_D65.xlsx        # Groundtruth under D65 lighting
â”œâ”€â”€ groundtruth_d75/
â”‚   â””â”€â”€ Sample1_D75.xlsx
â”œâ”€â”€ groundtruth_A/
â”‚   â””â”€â”€ Sample1_A.xlsx
â”œâ”€â”€ groundtruth_C/
â”‚   â””â”€â”€ Sample1_C.xlsx
â”œâ”€â”€ README_data.txt            # Sample data description
ðŸ”Ž Format Notes
input/1.xlsx: 49 rows Ã— 13 columns (1 wavelength column + 12 spectral columns)
Groundtruth files: 41 Ã— 49 excitation-emission matrices
See data/README_data.txt for full format details
âš ï¸ The full dataset (41 samples Ã— 4 modes) is not included.
To request full data access, please contact: your_email@example.com

ðŸ§  Model Overview
The main architecture consists of:

Input Embedding: Linear(6 â†’ 128) for both incident and emission vectors
Interaction Mechanism: Element-wise multiplication
1D CNN Layers: Capture local spectral features
Transformer Encoder: Capture long-range dependencies
Decoder: Fully connected layers with Softplus to predict non-negative EEM
Supported Models:
ExcitationSpectrumModel_CNN
ExcitationSpectrumModel_CNN_Transformer
ExcitationSpectrumModel_CNN_Transformer_Improved
ExcitationSpectrumModel_CNN_UNet
ExcitationSpectrumModel_CNN_ResNet
ExcitationSpectrumModel_CNN_MultiScale
ExcitationSpectrumModel_CNN_Interaction
ðŸ“Š Loss Functions
The total loss is a weighted combination of:

Mean Squared Error (MSE)
Generalized Feature Correlation (GFC Loss)
Smoothness Regularization
ðŸ” Loss function definitions are in: models/losses.py

ðŸ‹ï¸â€â™€ï¸ Training
To train the model:

python train.py
During training, the following will be saved:

Best model (.pth)
Loss history (.xlsx)
Loss curves (.png)
You can modify train.py to switch models or use only Sample 1 for testing:


X = X[1:2]
Y = Y[1:2]
ðŸ“ˆ Visualize Loss Curves

python plot_loss.py
Generates loss curve plots from training log.

ðŸ“¦ Requirements
Tested with:

Python â‰¥ 3.8
PyTorch â‰¥ 1.12
Dependencies:

torch>=1.12.1
pandas
numpy
scikit-learn
matplotlib
scipy
openpyxl
Install with:

pip install -r requirements.txt
ðŸ“Œ Notes
Input shape: [batch_size, 49, 13]
Output shape: [batch_size, 41, 49]
Output is non-negative due to Softplus in the decoder

